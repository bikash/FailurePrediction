temp = v[previousState,k-1] + log(hmm$transProbs[previousState,state])
maxi = max(maxi, temp)
}
v[state,k] = log(hmm$emissionProbs[state,observation[k]]) + maxi
}
}
# Traceback
viterbiPath = rep(NA,nObservations)
for(state in hmm$States)
{
if(max(v[,nObservations])==v[state,nObservations])
{
viterbiPath[nObservations] = state
break
}
}
for(k in (nObservations-1):1)
{
for(state in hmm$States)
{
if(max(v[,k]+log(hmm$transProbs[,viterbiPath[k+1]]))
==v[state,k]+log(hmm$transProbs[state,viterbiPath[k+1]]))
{
viterbiPath[k] = state
break
}
}
}
return(viterbiPath)
}
forward = function(hmm, observation)
{
hmm$transProbs[is.na(hmm$transProbs)]       = 0
hmm$emissionProbs[is.na(hmm$emissionProbs)] = 0
nObservations  = length(observation)
nStates    = length(hmm$States)
f          = array(NA,c(nStates,nObservations))
dimnames(f)= list(states=hmm$States,index=1:nObservations)
# Init
for(state in hmm$States)
{
f[state,1] = log(hmm$startProbs[state] * hmm$emissionProbs[state,observation[1]])
}
# Iteration
for(k in 2:nObservations)
{
for(state in hmm$States)
{
logsum = -Inf
for(previousState in hmm$States)
{
temp   = f[previousState,k-1] + log(hmm$transProbs[previousState,state])
if(temp > - Inf)
{
logsum = temp + log(1 + exp(logsum - temp ))
}
}
f[state,k] = log(hmm$emissionProbs[state,observation[k]]) + logsum
}
}
return(f)
}
backward = function(hmm, observation)
{
hmm$transProbs[is.na(hmm$transProbs)]       = 0
hmm$emissionProbs[is.na(hmm$emissionProbs)] = 0
nObservations  = length(observation)
nStates    = length(hmm$States)
b          = array(NA,c(nStates,nObservations))
dimnames(b)= list(states=hmm$States,index=1:nObservations)
# Init
for(state in hmm$States)
{
b[state,nObservations] = log(1)
}
# Iteration
for(k in (nObservations-1):1)
{
for(state in hmm$States)
{
logsum = -Inf
for(nextState in hmm$States)
{
temp   = b[nextState,k+1] + log(hmm$transProbs[state,nextState]*hmm$emissionProbs[nextState,observation[k+1]])
if(temp > - Inf)
{
logsum = temp + log(1 + exp(logsum-temp))
}
}
b[state,k] = logsum
}
}
return(b)
}
dishonestCasino = function()
{
# setup HMM
nSim          = 2000
States        = c("Fair","Unfair")
Symbols       = 1:6 #c("1er","2er","3er","4er","5er","6er")
transProbs    = matrix(c(.99,.01,.02,.98), c(length(States),length(States)), byrow=TRUE)
emissionProbs = matrix(c(rep(1/6,6),c(rep(.1,5),.5)), c(length(States),length(Symbols)), byrow=TRUE)
hmm = initHMM(States, Symbols, transProbs=transProbs, emissionProbs=emissionProbs)
sim = simHMM(hmm,nSim)
vit = viterbi(hmm, sim$observation)
f   = forward(hmm, sim$observation)
b   = backward(hmm, sim$observation)
baumWelch(hmm,sim$observation)
# todo: probObservations is not generic!
f[1,nSim]->i
f[2,nSim]->j
probObservations = (i + log(1+exp(j-i)))
posterior = exp((f+b)-probObservations)
x = list(hmm=hmm,sim=sim,vit=vit,posterior=posterior)
readline("Plot simulated throws:\n")
mn = "Fair and unfair die"
xlb = "Throw nr."
ylb = ""
plot(x$sim$observation,ylim=c(-7.5,6),pch=3,main=mn,xlab=xlb,ylab=ylb,bty="n",yaxt="n")
axis(2,at=1:6)
readline("Simulated, which die was used:\n")
text(0,-1.2,adj=0,cex=.8,col="black","True: green = fair die")
for(i in 1:nSim)
{
if(x$sim$states[i] == "Fair")
rect(i,-1,i+1,0, col = "green", border = NA)
else
rect(i,-1,i+1,0, col = "red", border = NA)
}
readline("Most probable path (viterbi):\n")
text(0,-3.2,adj=0,cex=.8,col="black","Most probable path")
for(i in 1:nSim)
{
if(x$vit[i] == "Fair")
rect(i,-3,i+1,-2, col = "green", border = NA)
else
rect(i,-3,i+1,-2, col = "red", border = NA)
}
readline("Differences:\n")
text(0,-5.2,adj=0,cex=.8,col="black","Difference")
differing = !(x$sim$states == x$vit)
for(i in 1:nSim)
{
if(differing[i])
rect(i,-5,i+1,-4, col = rgb(.3, .3, .3), border = NA)
else
rect(i,-5,i+1,-4, col = rgb(.9, .9, .9), border = NA)
}
readline("Posterior-probability:\n")
points(x$posterior[2,]-3, type="l")
#points(x$posterior[2,]-5, type="l")
readline("Difference with classification by posterior-probability:\n")
text(0,-7.2,adj=0,cex=.8,col="black","Difference by posterior-probability")
differing = !(x$sim$states == x$vit)
for(i in 1:nSim)
{
if(posterior[1,i]>0.5)
{
if(x$sim$states[i] == "Fair")
rect(i,-7,i+1,-6, col = rgb(.9, .9, .9), border = NA)
else
rect(i,-7,i+1,-6, col = rgb(.3, .3, .3), border = NA)
}else
{
if(x$sim$states[i] == "Unfair")
rect(i,-7,i+1,-6, col = rgb(.9, .9, .9), border = NA)
else
rect(i,-7,i+1,-6, col = rgb(.3, .3, .3), border = NA)
}
}
readline("Difference with classification by posterior-probability > .95:\n")
text(0,-7.2,adj=0,cex=.8,col="black","Difference by posterior-probability > .95")
differing = !(x$sim$states == x$vit)
for(i in 1:nSim)
{
if(posterior[2,i]>0.95 || posterior[2,i]<0.05)
{
if(differing[i])
rect(i,-7,i+1,-6, col = rgb(.3, .3, .3), border = NA)
else
rect(i,-7,i+1,-6, col = rgb(.9, .9, .9), border = NA)
}
else
{
rect(i,-7,i+1,-6, col = rgb(.9, .9, .9), border = NA)
}
}
invisible(x)
}
posterior = function(hmm, observation)
{
hmm$transProbs[is.na(hmm$transProbs)]       = 0
hmm$emissionProbs[is.na(hmm$emissionProbs)] = 0
f = forward(hmm, observation)
b = backward(hmm, observation)
probObservations = f[1,length(observation)]
for(i in 2:length(hmm$States))
{
j = f[i,length(observation)]
if(j > - Inf)
{
probObservations = j + log(1+exp(probObservations-j))
}
}
posteriorProb = exp((f+b)-probObservations)
return(posteriorProb)
}
baumWelch = function(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
{
tempHmm = hmm
tempHmm$transProbs[is.na(hmm$transProbs)]       = 0
tempHmm$emissionProbs[is.na(hmm$emissionProbs)] = 0
diff = c()
for(i in 1:maxIterations)
{
# Expectation Step (Calculate expected Transitions and Emissions)
bw = baumWelchRecursion(tempHmm, observation)
T  = bw$TransitionMatrix
E  = bw$EmissionMatrix
# Pseudocounts
T[!is.na(hmm$transProbs)]    = T[!is.na(hmm$transProbs)]    + pseudoCount
E[!is.na(hmm$emissionProbs)] = E[!is.na(hmm$emissionProbs)] + pseudoCount
# Maximization Step (Maximise Log-Likelihood for Transitions and Emissions-Probabilities)
T = (T/apply(T,1,sum))
E = (E/apply(E,1,sum))
d = sqrt(sum((tempHmm$transProbs-T)^2)) + sqrt(sum((tempHmm$emissionProbs-E)^2))
diff = c(diff, d)
tempHmm$transProbs    = T
tempHmm$emissionProbs = E
if(d < delta)
{
break
}
}
tempHmm$transProbs[is.na(hmm$transProbs)]       = NA
tempHmm$emissionProbs[is.na(hmm$emissionProbs)] = NA
return(list(hmm=tempHmm,difference=diff))
}
baumWelchRecursion = function(hmm, observation)
{
TransitionMatrix    = hmm$transProbs
TransitionMatrix[,] = 0
EmissionMatrix      = hmm$emissionProbs
EmissionMatrix[,]   = 0
f = forward(hmm,  observation)
b = backward(hmm, observation)
probObservations = f[1,length(observation)]
for(i in 2:length(hmm$States))
{
j = f[i,length(observation)]
if(j > - Inf)
{
probObservations = j + log(1+exp(probObservations-j))
}
}
for(x in hmm$States)
{
for(y in hmm$States)
{
temp = f[x,1] + log(hmm$transProbs[x,y]) +
log(hmm$emissionProbs[y,observation[1+1]]) + b[y,1+1]
for(i in 2:(length(observation)-1))
{
j = f[x,i] + log(hmm$transProbs[x,y]) +
log(hmm$emissionProbs[y,observation[i+1]]) + b[y,i+1]
if(j > - Inf)
{
temp = j + log(1+exp(temp-j))
}
}
temp = exp(temp - probObservations)
TransitionMatrix[x,y] = temp
}
}
for(x in hmm$States)
{
for(s in hmm$Symbols)
{
temp = -Inf
for(i in 1:length(observation))
{
if(s == observation[i])
{
j = f[x,i] + b[x,i]
if(j > - Inf)
{
temp = j + log(1+exp(temp-j))
}
}
}
temp = exp(temp - probObservations)
EmissionMatrix[x,s] = temp
}
}
return(list(TransitionMatrix=TransitionMatrix,EmissionMatrix=EmissionMatrix))
}
viterbiTraining = function(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
{
tempHmm = hmm
tempHmm$transProbs[is.na(hmm$transProbs)]       = 0
tempHmm$emissionProbs[is.na(hmm$emissionProbs)] = 0
diff = c()
for(i in 1:maxIterations)
{
# Counts
vt = viterbiTrainingRecursion(tempHmm, observation)
T  = vt$TransitionMatrix
E  = vt$EmissionMatrix
# Pseudocounts
T[!is.na(hmm$transProbs)]    = T[!is.na(hmm$transProbs)]    + pseudoCount
E[!is.na(hmm$emissionProbs)] = E[!is.na(hmm$emissionProbs)] + pseudoCount
# Relativ Frequencies
T = (T/apply(T,1,sum))
E = (E/apply(E,1,sum))
d = sqrt(sum((tempHmm$transProbs-T)^2)) + sqrt(sum((tempHmm$emissionProbs-E)^2))
diff = c(diff, d)
tempHmm$transProbs    = T
tempHmm$emissionProbs = E
if(d < delta)
{
break
}
}
tempHmm$transProbs[is.na(hmm$transProbs)]       = NA
tempHmm$emissionProbs[is.na(hmm$emissionProbs)] = NA
return(list(hmm=tempHmm,difference=diff))
}
viterbiTrainingRecursion = function(hmm, observation)
{
TransitionMatrix    = hmm$transProbs
TransitionMatrix[,] = 0
EmissionMatrix      = hmm$emissionProbs
EmissionMatrix[,]   = 0
v = viterbi(hmm,  observation)
for(i in 1:(length(observation)-1))
{
TransitionMatrix[v[i],v[i+1]] = TransitionMatrix[v[i],v[i+1]] + 1
}
for(i in 1:length(observation))
{
EmissionMatrix[v[i],observation[i]] = EmissionMatrix[v[i],observation[i]] + 1
}
return(list(TransitionMatrix=TransitionMatrix,EmissionMatrix=EmissionMatrix))
}
dishonestCasino()
##Author: Bikash Agrawal
##Description: Create final result from log, or pre-processing of log file.
library(lattice)
library(plyr)
library(depmixS4)
library(TTR) # For downloading SP500 index
library(ggplot2)
library(reshape2)
library(xts)
dir = "/Users/bikash/repos/FailurePrediction/R" # path for macbots1ok
dir = "/home/bikash/repos/FailurePrediction/R" # path in linux machine
setwd(dir)
##Plot number of observation Error Vs time
Data1 = read.table("file/error_25.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data1$day <- cut(as.POSIXlt( Data1$date,  origin="1970-01-01" ), breaks = "day")
getcount <- function(Df) { c(count = as.numeric(length(Df$ErrorType)),
obs = paste(Df$ErrorType, collapse=","))
}
ts1 <- ddply(Data1, .(day),getcount)
## Data from haisen 26
Data2 = read.table("file/error_26.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data2$day <- cut(as.POSIXlt( Data2$date,  origin="1970-01-01" ), breaks = "day")
ts2 <- ddply(Data2, .(day),getcount)
## Data from haisen 27
Data3 = read.table("file/error_27.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data3$day <- cut(as.POSIXlt( Data3$date,  origin="1970-01-01" ), breaks = "day")
ts3 <- ddply(Data3, .(day),getcount)
## Data from haisen 28
Data4 = read.table("file/error_28.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data4$day <- cut(as.POSIXlt( Data4$date,  origin="1970-01-01" ), breaks = "day")
ts4 <- ddply(Data4, .(day),getcount)
## Data from haisen 29
Data5 = read.table("file/error_29.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data5$day <- cut(as.POSIXlt( Data5$date,  origin="1970-01-01" ), breaks = "day")
ts5 <- ddply(Data5, .(day),getcount)
## d data from haisen22
Data6 = read.table("file/error_22.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data6$day <- cut(as.POSIXlt( Data6$date,  origin="1970-01-01" ), breaks = "day")
ts6 <- ddply(Data6, .(day),getcount)
## d data from haisen20
Data7 = read.table("file/error_20.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data7$day <- cut(as.POSIXlt( Data7$date,  origin="1970-01-01" ), breaks = "day")
ts7 <- ddply(Data7, .(day),getcount)
# plot graph
x1 = c(1:21)
y1 = ts1$count
x2 = x1
y2 = ts2$count
x3 = x1
y3 = ts3$count
x4 = x1
y4 = ts4$count
x5 = x1
y5 = ts5$count
x = ts1$day
library(lattice)
library(plyr)
library(depmixS4)
library(TTR) # For downloading SP500 index
library(ggplot2)
library(reshape2)
library(xts)
dir = "/Users/bikash/repos/FailurePrediction/R" # path for macbots1ok
#dir = "/home/bikash/repos/FailurePrediction/R" # path in linux machine
setwd(dir)
##Plot number of observation Error Vs time
Data1 = read.table("file/error_25.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data1$day <- cut(as.POSIXlt( Data1$date,  origin="1970-01-01" ), breaks = "day")
getcount <- function(Df) { c(count = as.numeric(length(Df$ErrorType)),
obs = paste(Df$ErrorType, collapse=","))
}
ts1 <- ddply(Data1, .(day),getcount)
## Data from haisen 26
Data2 = read.table("file/error_26.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data2$day <- cut(as.POSIXlt( Data2$date,  origin="1970-01-01" ), breaks = "day")
ts2 <- ddply(Data2, .(day),getcount)
## Data from haisen 27
Data3 = read.table("file/error_27.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data3$day <- cut(as.POSIXlt( Data3$date,  origin="1970-01-01" ), breaks = "day")
ts3 <- ddply(Data3, .(day),getcount)
## Data from haisen 28
Data4 = read.table("file/error_28.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data4$day <- cut(as.POSIXlt( Data4$date,  origin="1970-01-01" ), breaks = "day")
ts4 <- ddply(Data4, .(day),getcount)
## Data from haisen 29
Data5 = read.table("file/error_29.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data5$day <- cut(as.POSIXlt( Data5$date,  origin="1970-01-01" ), breaks = "day")
ts5 <- ddply(Data5, .(day),getcount)
## d data from haisen22
Data6 = read.table("file/error_22.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data6$day <- cut(as.POSIXlt( Data6$date,  origin="1970-01-01" ), breaks = "day")
ts6 <- ddply(Data6, .(day),getcount)
## d data from haisen20
Data7 = read.table("file/error_20.txt",
sep=";",
col.names=c("date",  "status", "ErrorType", "Node"),
fill=FALSE,
strip.white=TRUE)
Data7$day <- cut(as.POSIXlt( Data7$date,  origin="1970-01-01" ), breaks = "day")
ts7 <- ddply(Data7, .(day),getcount)
# plot graph
x1 = c(1:21)
y1 = ts1$count
x2 = x1
y2 = ts2$count
x3 = x1
y3 = ts3$count
x4 = x1
y4 = ts4$count
x5 = x1
y5 = ts5$count
x = ts1$day
}
loglikelihood=function(par,type,x,start,m,T,freq=1,fct_dmat,fct_gamma,                       fct_delta,ddl,dml,parameters,debug=FALSE){# Arguments:# par: vector of parameter values for log-likelihood evaluation# type: vector of parameter names used to split par vector into types# x: matrix of observed sequences (row:id; column:occasion/time)# start: matrix with a row for each id and two columns# 1) first observed state, 2) first occasion observed# m: number of states# T: number of occasions; sequence length# freq: vector of history frequencies or 1# fct_dmat: function to create D from parameters# fct_gamma: function to create gamma - transition matrix# fct_delta: function to create initial state probability distribution matrix# ddl: design data list of parameters for each id# model: formulas for each parameter type# Other variables:# parlist: list of parameter vectors split by type (eg Phi, p in CJS)# gamma: array of transition matrices - one for each id, time# dmat: array of observation probability matrices - one for each id, time## Create list of parameter matrices from single input parameter vector# First split parameter vector by prameter type (type)parlist=split(par,type)pars=list()# For each parameter type call function reals to compute vector# of real parameter values; then use laply and split to create# a matrix of parameter values with a row for each id and column for# each occasion.for(parname in names(parameters)){  R=reals(ddl=ddl[[parname]],dml=dml[[parname]],        parameters=parameters[[parname]],parlist=parlist[[parname]])pars[[parname]]=laply(split(R,ddl[[parname]]$id),function(x) x)}# compute four dimensional arrays of id- and occasion-specific# observation and transition matrices using parameter valuesdmat=fct_dmat(pars,m,F=start[,2],T)gamma=fct_gamma(pars,m,F=start[,2],T)# compute matrix of initial state distribution for each iddelta=fct_delta(pars,m,F=start[,2],T,start)# loop over each encounter history in sapply and# create log-likelihood vector - an element for each x# sum is total log-likelihood across individuals# return negative log-likelihoodneglnl=-sum(freq*sapply(1:nrow(x),function(id)  HMMLikelihood(x[id,],start[id,2],m,T,                dmat=dmat[id,,,],gamma=gamma[id,,,],                delta=delta[id,])))return(neglnl)}reals=function(ddl,dml,parameters,parlist) { # Computes real estimates for HMM models using inverse of# link from design matrix and for a particular parameter# type (parname); handles fixed parameters assigned by    # non-NA value in field named fix in the ddl dataframe.  dm=dml$fe  # Currently for log,logit or identity link, return the inverse values  values=switch(parameters$link,                log=exp(as.vector(dm%*%parlist)),                logit=plogis(as.vector(dm%*%parlist)),                identity=as.vector(dm%*%parlist))  if(!is.null(ddl$time.interval))values=values^ddl$time.interval  # if some reals are fixed, set reals to their fixed values  if(!is.null(ddl$fix))    values[!is.na(ddl$fix)]=ddl$fix[!is.na(ddl$fix)]  # return vector of reals  return(values)}cjs_dmat=function(pars,m,F,T) {# add first occasion p=1pmat=array(NA,c(nrow(pars$p),T,2,2))for (i in 1:nrow(pmat))  {  pmat[i,F[i],,]=matrix(c(0,1,1,0),nrow=2,ncol=2,byrow=TRUE)  for(j in F[i]:(T-1))    f  p=pars$p[i,j]  pmat[i,j+1,,]=matrix(c(1-p,1,p,0),nrow=2,ncol=2,byrow=TRUE)  g  g  pmat  g  cjs_gamma=function(pars,m,F,T)    f  # create four dimensional (4-d) array with a matrix for each id and occasion  # from pars$Phi which is a matrix of id by occasion survival probabilities  phimat=array(NA,c(nrow(pars$Phi),T-1,m,m))  for (i in 1:nrow(phimat))    for(j in F[i]:(T-1))      {  phi=pars$Phi[i,j]  phimat[i,j,,]=matrix(c(phi,1-phi,0,1),nrow=2,ncol=2,byrow=TRUE)    }  phimat}  cjs_delta=function(pars,m,F,T,start)   {   if(is.list(m))m=m$ns*m$na+1  delta=matrix(0,nrow=nrow(start),ncol=m)  delta[cbind(1:nrow(start),start[,1])]=1  delta  }
loglikelihood=function(par,type,x,start,m,T,freq=1,fct_dmat,fct_gamma,                       fct_delta,ddl,dml,parameters,debug=FALSE){# Arguments:# par: vector of parameter values for log-likelihood evaluation# type: vector of parameter names used to split par vector into types# x: matrix of observed sequences (row:id; column:occasion/time)# start: matrix with a row for each id and two columns# 1) first observed state, 2) first occasion observed# m: number of states# T: number of occasions; sequence length# freq: vector of history frequencies or 1# fct_dmat: function to create D from parameters# fct_gamma: function to create gamma - transition matrix# fct_delta: function to create initial state probability distribution matrix# ddl: design data list of parameters for each id# model: formulas for each parameter type# Other variables:# parlist: list of parameter vectors split by type (eg Phi, p in CJS)# gamma: array of transition matrices - one for each id, time# dmat: array of observation probability matrices - one for each id, time## Create list of parameter matrices from single input parameter vector# First split parameter vector by prameter type (type)parlist=split(par,type)pars=list()# For each parameter type call function reals to compute vector# of real parameter values; then use laply and split to create# a matrix of parameter values with a row for each id and column for# each occasion.for(parname in names(parameters)){  R=reals(ddl=ddl[[parname]],dml=dml[[parname]],        parameters=parameters[[parname]],parlist=parlist[[parname]])pars[[parname]]=laply(split(R,ddl[[parname]]$id),function(x) x)}# compute four dimensional arrays of id- and occasion-specific# observation and transition matrices using parameter valuesdmat=fct_dmat(pars,m,F=start[,2],T)gamma=fct_gamma(pars,m,F=start[,2],T)# compute matrix of initial state distribution for each iddelta=fct_delta(pars,m,F=start[,2],T,start)# loop over each encounter history in sapply and# create log-likelihood vector - an element for each x# sum is total log-likelihood across individuals# return negative log-likelihoodneglnl=-sum(freq*sapply(1:nrow(x),function(id)  HMMLikelihood(x[id,],start[id,2],m,T,                dmat=dmat[id,,,],gamma=gamma[id,,,],                delta=delta[id,])))return(neglnl)}reals=function(ddl,dml,parameters,parlist) { # Computes real estimates for HMM models using inverse of# link from design matrix and for a particular parameter# type (parname); handles fixed parameters assigned by    # non-NA value in field named fix in the ddl dataframe.  dm=dml$fe  # Currently for log,logit or identity link, return the inverse values  values=switch(parameters$link,                log=exp(as.vector(dm%*%parlist)),                logit=plogis(as.vector(dm%*%parlist)),                identity=as.vector(dm%*%parlist))  if(!is.null(ddl$time.interval))values=values^ddl$time.interval  # if some reals are fixed, set reals to their fixed values  if(!is.null(ddl$fix))    values[!is.na(ddl$fix)]=ddl$fix[!is.na(ddl$fix)]  # return vector of reals  return(values)}cjs_dmat=function(pars,m,F,T) {# add first occasion p=1pmat=array(NA,c(nrow(pars$p),T,2,2))for (i in 1:nrow(pmat))  {  pmat[i,F[i],,]=matrix(c(0,1,1,0),nrow=2,ncol=2,byrow=TRUE)  for(j in F[i]:(T-1))    {  p=pars$p[i,j]  pmat[i,j+1,,]=matrix(c(1-p,1,p,0),nrow=2,ncol=2,byrow=TRUE)  }}  pmat}  cjs_gamma=function(pars,m,F,T)    {  # create four dimensional (4-d) array with a matrix for each id and occasion  # from pars$Phi which is a matrix of id by occasion survival probabilities  phimat=array(NA,c(nrow(pars$Phi),T-1,m,m))  for (i in 1:nrow(phimat))    for(j in F[i]:(T-1))      {  phi=pars$Phi[i,j]  phimat[i,j,,]=matrix(c(phi,1-phi,0,1),nrow=2,ncol=2,byrow=TRUE)    }  phimat}  cjs_delta=function(pars,m,F,T,start)   {   if(is.list(m))m=m$ns*m$na+1  delta=matrix(0,nrow=nrow(start),ncol=m)  delta[cbind(1:nrow(start),start[,1])]=1  delta  }
